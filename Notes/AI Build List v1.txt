AI research & self teaching

    Differentiate between supervised learning & reinforcement learning
        Supervised learning - learning by example
            Here are examples. Learn patterns from these examples

        Reinforcement learning - learning by experience
            Here is an environment. Learn patterns by exploring it

    Understand deep reinforcement learning technique(s) will be needed for this implementation
        Trial & error system
        Incorporated with neural networks

Notes
    How to provide a reward without the agent needing to be finished for faster more effective training
        Reason
            Model in current build will only receive a reward for finishing,
            so until it finishes it won't know what its doing correct and will most likely never finish
            By providing a secondary reward for every run, the model can adjust for every training iteration
        Possible Fixes (Form of secondary reward)
            Time on track
                The longer the agent stays on track the larger reward it will receive.
                Problem here is it will learn to just no move until score zeros out to get max reward
            Distance traveled
                The further the agent travels the larger reward received
                This is better as the model will strive for the agent to move further along the track with every iteration
                Problem may include, agent cheating the system by going back and forth as direction of distance is not implied
                Hope that naturally agent can get higher score by only moving along track rather than moving along a straight
                doing a 180 and going back as score will zero out eventually
        More
            Secondary score should be weighted less than finishing as finishing is the primary goal

Simple NN Modelling to do
    Goal is to create a simple DRL NN Model
    Provisional List
        This consists of inputs in form of a numpy array
        Forwards Prop through hidden layers
        Backwards Prop
        Loss functions
        Way to provide model a reward (is this per state or per track attempt)
        Future possibilities for Q-Learning and other reinforcement learning algorithms

    Collate raw inputs in a numpy array to be passed to the NN Model - DONE
        Provisional shape of input matrix determined is (6, 2)

    Create spawn loop for model so when track attempt is over (agent completed/died) it spawns in again to a random track
        Provisionally will spawn to same track as track generation has been excluded for modelling
        At track completion store results value in a vector (2, 1) -> (score, secondary reward 'distance travelled')

    Create overall simple NN model
        Determine Input layer - including activation function
        Determine number of hidden layers
        Determine how output will execute 2 of six options

